{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Loading and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = \"CrowdstormingDataJuly1st.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each row in the dataset is a (player _p_, referee _r_) dyad containing some information about the player, total number of each type of card (yellow, yellow-red, and red) given by _r_ to _p_, and some statistics about racial bias in the referee's home country.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of entries: %d\" % len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We want to make sure we do our analysis with a clean dataset, so we check for null entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that there are many players without skin color ratings. Those players aren't going to be useful for our prediction task, so we drop all of them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"rater1\", \"rater2\"])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What other missing values are we dealing with in this dataset?**\n",
    "- Position: We see that of over 100,000 dyads there are several thousand rows missing the position field. We choose not to modify these values because we aren't going to use the player's position for our prediction.\n",
    "- Height/weight and bias information for referee country: only a few hundred rows are missing this information, so we choose to drop all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['height', 'weight', 'Alpha_3', 'meanIAT', 'nIAT', 'seIAT', 'meanExp', 'nExp', 'seExp'])\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK, now we have a clean dataset to work with!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We read through the Crowdstorming analytics visualization notebook linked in the homework description. Unlike them, we decided that we do not have a strong reason to drop the dyads that they judged \"incomplete\" (i.e. did not have at least 11 referee-player dyads from the same club).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We decided to only work with the numeric features in the dataset, such as:\n",
    "height, weight, game statistics, goals, cards, skin color ratings, and referee statistics.\n",
    "We may be able to achieve better accuracy by encoding categorical features like the position and the country but for this assignment we chose to focus on playing with the classifier rather than going for maximum accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How we apparently compute our features:\n",
    "# Group by player, take the mean over the dyad\n",
    "# This doesn't make much sense for total game stats,\n",
    "# but the average number goals per game, cards per game, \n",
    "# and most importantly meanIAT and meanExp of the referees\n",
    "# over all the player's games are useful\n",
    "df_by_player = df.groupby(\"playerShort\")\n",
    "df_players = df_by_player.agg(np.mean)\n",
    "df_players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a df with player constant description attributes\n",
    "df_players_description = pd.DataFrame(df_players[[\"height\", \"weight\", \"rater1\", \"rater2\"]])\n",
    "df_players_description.sample(5)\n",
    "len(df_players_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raters consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that the raters have a certain bias and do not always rate the same player the same way. We look at the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(df_players_description[\"rater1\"] - df_players_description[\"rater2\"]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We see that rater2 rates the skintone higher than rater1 on average. \n",
    "We now make a new attribute that is the mean of rater1 and rater2's scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players_description[\"rateMean\"] = (df_players_description[\"rater1\"] + df_players_description[\"rater2\"]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players_description.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since random forest uses binary classification, we decided to define a binary attribute \"darkSkin\". We have to choose the limit between \"white\" and \"black\". We arbitrarily chose a mean rating greater than or equal to 0.5 to be considered \"darkSkin\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_players_description['darkSkin'] = df_players_description['rateMean']  >= 0.5\n",
    "df_players_description.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df_players_description.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_players_description =  df_players_description.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we train a RandomForestClassifier that given almost all of the features (except skin color ratings of course) outputs his skin color. We'll show that using the default settings it overfits to the training set, then try again with cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players.head()\n",
    "df_players['darkSkin'] = df_players_description['darkSkin']\n",
    "\n",
    "X_all = df_players.drop(['rater1', 'rater2', 'darkSkin'], axis=1) # not in-place\n",
    "y_all = df_players_description['darkSkin']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def print_metrics(y, y_pred):\n",
    "    # Print out some metrics judging the quality of the classification\n",
    "    print(\"Accuracy:\")\n",
    "    print(metrics.accuracy_score(y, y_pred))\n",
    "    print(\"F1: \")\n",
    "    print(metrics.f1_score(y, y_pred))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y, y_pred))\n",
    "    print(\"-------\")\n",
    "\n",
    "# Split the data into 60% of training set, and 40% of test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.4)\n",
    "\n",
    "clf_all = RandomForestClassifier()\n",
    "clf_all.fit(X_train, y_train)\n",
    "y_pred_train = clf_all.predict(X_train)\n",
    "y_pred_test = clf_all.predict(X_test)\n",
    "print('Accuracy on training set is {0:.2f}%'.format(metrics.accuracy_score(y_train, y_pred_train)*100))\n",
    "print_metrics(y_train, y_pred_train)\n",
    "print('Accuracy on testing set is {0:.2f}%'.format(metrics.accuracy_score(y_test, y_pred_test)*100))\n",
    "print_metrics(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, we obtain a nearly perfect classification accuracy on the training set but much worse on the test set. This isn't too surprising. With our small amount of data the random forest is likely overfitting to the jumble of numeric features we gave it. We will try reducing the maximum allowed height of the trees and see how the accuracy changes:**\n",
    "- max_depth: parameter governing height of the trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_shorter = RandomForestClassifier(max_depth=5)\n",
    "\n",
    "clf_shorter.fit(X_train, y_train)\n",
    "y_pred_train_shorter = clf_shorter.predict(X_train)\n",
    "y_pred_test_shorter = clf_shorter.predict(X_test)\n",
    "\n",
    "print('Accuracy on training set is {0:.2f}%'.format(metrics.accuracy_score(y_train, y_pred_train_shorter)*100))\n",
    "print_metrics(y_train, y_pred_train_shorter)\n",
    "print('Accuracy on testing set is {0:.2f}%'.format(metrics.accuracy_score(y_test, y_pred_test_shorter)*100))\n",
    "print_metrics(y_test, y_pred_test_shorter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When we only allow short trees (max_depth <= 5) in our random forest it seems to reduce the overfitting issue, based on the improved accuracy and F1 score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next part: We start again with random forest, this time using cross validation.\n",
    "Let's start with a simple model: considering only height and weight, try to obtain the player's skin color.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df_players_description[['height', 'weight']]\n",
    "y = df_players_description['darkSkin']\n",
    "clf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "y_pred = clf.predict(X)\n",
    "print('The accuracy is {0:.2f}%'.format(metrics.accuracy_score(y, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  81% accuracy can seem pretty good, but we must remember that we are training on the whole dataset so it doesn't mean much. If we would try to predict on unseen data, the result would be poor as we are probably overfitting the training set. Now let's use cross-validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: one example with train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split the data into 60% of training set, and 40% of test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('The accuracy is {0:.2f}%'.format(metrics.accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Training on 60% of the data gives around 70% accuracy. Let's try with a 20-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X, y, cv=20, scoring='accuracy')\n",
    "pd.Series(scores).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median score is 72%, and the standard deviation is not too high. That a decent score, considering that we are only looking at two features: height and weight. Let's try to add game features and see how the score changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_forest_scores(dataframe, features, target=\"darkSkin\", estimators=100, folds=20):\n",
    "    clf = RandomForestClassifier(n_estimators=estimators)\n",
    "    X = dataframe[features]\n",
    "    y = list(dataframe[\"darkSkin\"].values)\n",
    "    \n",
    "    # Cross validation scores\n",
    "    scores = cross_val_score(clf, X, y, cv=folds, scoring='accuracy')\n",
    "    \n",
    "    # Train again to get feature importances\n",
    "    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.4)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    return scores, clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players = df_players.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible_features = ['height', 'weight', 'games', 'victories', 'ties', 'defeats', 'goals', 'yellowCards', 'yellowReds', 'redCards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores, importances = random_forest_scores(df_players, possible_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(scores).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding features adds around 3% of precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(possible_features, importances):\n",
    "    df_feature_importances = pd.DataFrame({\"features\": possible_features, \"importances\": importances})\n",
    "    df_feature_importances = df_feature_importances.set_index(\"features\")\n",
    "    df_feature_importances = df_feature_importances.sort_values(\"importances\", ascending=False)\n",
    "    df_feature_importances.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(possible_features, importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "possible_features_2 = list(possible_features)\n",
    "possible_features_2.remove(\"goals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores2, importances2 = random_forest_scores(df_players, possible_features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(scores2).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(scores).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now try to incorporate the information about racial bias in each referee's country of origin.**\n",
    "\n",
    "Previously we computed the average meanIAT and meanExp values for each player over all referee-player dyads. We again use the existing meanIAT and meanExp information from each dyad to create two new features for each player: stdIAT and stdExp. Note that this is not the same calculation as averaging the standard deviations _seIAT_ and _seExp_ from each dyad - on a high-level stdIAT and stdExp indicate the variance in referee nationalities (and potentially biases) over  all matches in which the player played.\n",
    "\n",
    "This gives us four racial bias-related features:\n",
    "- meanIAT = average of meanIAT values over all dyads containing the given player\n",
    "- meanExp = average of meanExp values over all dyads containing the given player\n",
    "- stdIAT = std deviation of meanIAT values over all dyads containing the given player. General idea: has this player been referee'd by a diverse group of referees?\n",
    "- stdExp = std deviation of meanExp values over all dyads containing the given player\n",
    "\n",
    "**We also computed a meanIAT_carded features, which is the average of meanIAT values over all dyads where the player received at least one card from that referee. But we commented it out because it didn't seem to work well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Not going to use this feature after all (meanIAT_carded)\n",
    "#df['meanIAT_carded'] = df.apply(lambda x: x.meanIAT*(x.yellowCards+x.yellowReds+x.redCards), axis=1)\n",
    "#df_players[['stdIAT','stdExp', 'stdIAT_carded']] = df_by_player['meanIAT', 'meanExp', 'meanIAT_carded'].agg(np.std)\n",
    "df_players[['stdIAT','stdExp']] = df_by_player['meanIAT', 'meanExp'].agg(np.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We check that our new standard deviation features were computed correctly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players[df_players['stdExp'].isnull()]#.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players[df_players['stdIAT'].isnull()]#.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that there are some NaN values so we replace them with 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players.replace(to_replace={'stdIAT': {np.nan : 0}, 'stdExp': {np.nan : 0}, 'stdIAT_carded': {np.nan : 0}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#possible_features_with_bias = ['stdIAT_carded', 'meanIAT_carded', 'meanIAT', 'stdIAT', 'meanExp', 'stdExp', 'height', 'weight', 'games', 'victories', 'ties', 'defeats', 'goals', 'yellowCards', 'yellowReds', 'redCards']\n",
    "possible_features_with_bias = ['meanIAT', 'meanExp', 'height', 'weight', 'games', 'victories', 'ties', 'defeats', 'goals', 'yellowCards', 'yellowReds', 'redCards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores_bias, importances_bias = random_forest_scores(df_players, possible_features_with_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(scores_bias).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(possible_features_with_bias, importances_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When we include the features relating to the racial bias in referee countries, we see a bump of several percent in accuracy, up to 78-79%. These features are ranked highly in importance so they seem to be quite important to the prediction!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose KMeans as clustering algorithm as it is the simplest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the features corresponding to national racial bias in the referee's home countries, since in part 1 we found these to be predictive of skin color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias_features = ['meanIAT', 'stdIAT', 'meanExp', 'stdExp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df_players[bias_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run KMeans with the default k-means++ initialization strategy. We're looking for two clusters corresponding to dark skinned and light skinned players respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2).fit(X)\n",
    "cluster_indexes = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster1 = df_players.loc[cluster_indexes == 0]\n",
    "cluster2 = df_players.loc[cluster_indexes == 1]\n",
    "\n",
    "def dark_skin_proportion(cluster):\n",
    "    \"\"\"Return the proportion of dark skin player in the cluster\"\"\"\n",
    "    return cluster['darkSkin'].sum() / len(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dark_skin_proportion(cluster1), dark_skin_proportion(cluster2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_dark_separator_score(cluster1, cluster2):\n",
    "    \"\"\"Returns a heuristic that gives a score between 0 and 1 to the cluster assignment.\n",
    "    The higher the score, the more the clusters separate players by skin colors\"\"\"\n",
    "    return np.abs(dark_skin_proportion(cluster1) - dark_skin_proportion(cluster2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_dark_separator_score(cluster1, cluster2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmean_dark_separator_score(X):\n",
    "    \"\"\"Given a dataset, returns heuristic score and silhouette score for the\n",
    "    clusters generated by KMeans\"\"\"\n",
    "    kmeans = KMeans(n_clusters=2).fit(X)\n",
    "    cluster_indexes = kmeans.predict(X)\n",
    "    cluster1 = df_players.loc[cluster_indexes == 0]\n",
    "    cluster2 = df_players.loc[cluster_indexes == 1]\n",
    "    \n",
    "    \n",
    "    return cluster_dark_separator_score(cluster1, cluster2), silhouette_score(X, cluster_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximize score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at all 1-feature cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_score_silhouette = []\n",
    "\n",
    "for feature in possible_features_with_bias:\n",
    "    score, silhouette = kmean_dark_separator_score(df_players[[feature]])\n",
    "    feature_score_silhouette.append((feature, score, silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the 1-feature case, the silhouette is always between 0.5-0.75 which is quite good. We thus order by the score they give when they are used alone in KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_score_silhouette = sorted(feature_score_silhouette, key=lambda x:x[1])\n",
    "[\"Feature: {:12} Score: {:.2f}    Silhouette: {:.2f}\".format(feature, score, silhouette) \n",
    "     for feature, score, silhouette in reversed(feature_score_silhouette)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We order the features by score they give as a single KMean feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ordered_features = [fscore[0] for fscore in feature_score_silhouette]\n",
    "\",\".join(ordered_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove each feature iteratively and print the resulting heuristic score and silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove each feature iteratively\n",
    "for i in range(len(ordered_features)):\n",
    "    score, silhouette = kmean_dark_separator_score(df_players[ordered_features[i:]])\n",
    "    \n",
    "    print('Before removing {:12} score is {:.02f}  silhouette is {:.02f}'.format(ordered_features[i], score, silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that keeping only `[\"meanIAT\", \"stdIAT\"]` is optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df_players[[\"meanIAT\", \"stdIAT\"]]\n",
    "kmeans = KMeans(n_clusters=2).fit(X)\n",
    "cluster_indexes = kmeans.predict(X)\n",
    "cluster1 = df_players.loc[cluster_indexes == 0]\n",
    "cluster2 = df_players.loc[cluster_indexes == 1]\n",
    "\n",
    "FMT = \"Cluster {}: {:.2f}% of dark skin players\"\n",
    "print(FMT.format(1, 100 * dark_skin_proportion(cluster1)))\n",
    "print(FMT.format(2, 100 * dark_skin_proportion(cluster2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the clustering works decently, one cluster contains **a vast majority of dark skin players** while the other contains a **vast majority of light skin players**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the silhouette score is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"Silhouette score: {:.2f}\".format(silhouette_score(X, cluster_indexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We thus find that using features `[\"meanIAT\", \"stdIAT\"]` gives two clusters with good silhouette score, and separates quite well dark-skin and light-skin players**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
