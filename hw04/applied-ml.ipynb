{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = \"CrowdstormingDataJuly1st.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each row in the dataset is a (player _p_, referee _r_) dyad containing some information about the player, total number of each type of card (yellow, yellow-red, and red) given by _r_ to _p_, and some statistics about racial bias in the referee's home country.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of entries: %d\" % len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We want to make sure we do our analysis with a clean dataset, so we check for null entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that there are many players without skin color ratings. Those players aren't going to be useful for our prediction, so we drop all of them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"rater1\", \"rater2\"])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_by_player = df.groupby(\"playerShort\")\n",
    "df_players = df_by_player.agg(np.mean)\n",
    "df_players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a df with player constant description attributes\n",
    "df_players_description = pd.DataFrame(df_players[[\"height\", \"weight\", \"rater1\", \"rater2\"]])\n",
    "df_players_description.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raters consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that the raters have a certain bias and do not always rate the same player the same way. We look at the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(df_players_description[\"rater1\"] - df_players_description[\"rater2\"]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We see that rater2 rates the skintone higher than rater1 on average. \n",
    "We now make a new attribute that is the mean of rater1 and rater2's scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players_description[\"rateMean\"] = (df_players_description[\"rater1\"] + df_players_description[\"rater2\"]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players_description.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since random forest uses categorical classification, and since we decided to have a binary attribute \"darkSkin\", we need to choose the limit between \"white\" and \"black\". We arbitrarily chose mean rate equal and over 0.5 to be considered \"black\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_players_description['darkSkin'] = df_players_description['rateMean']  >= 0.5\n",
    "df_players_description.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df_players_description.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_players_description =  df_players_description.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a simple machine learning task: considering only height and weight, try to obtain the player's skin color. For that we use a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = df_players_description[['height', 'weight']]\n",
    "y = df_players_description['darkSkin']\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = clf.predict(X)\n",
    "print('The accuracy is {0:.2f}%'.format(metrics.accuracy_score(y, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  81% accuracy can seem pretty good, but we must remember that we are training on the whole dataset so it doesn't mean much. If we would try to predict on unseen data, the result would be poor as we are probably overfitting the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split the data into 60% of training set, and 40% of test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('The accuracy is {0:.2f}%'.format(metrics.accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Training on 60% of the data gives around 70% accuracy, which is still good. Let's try with a 20-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X, y, cv=20, scoring='accuracy')\n",
    "pd.Series(scores).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median score is 72%, and the standard deviation is not too high. That a decent score, considering that we are only looking at two features: height and weight. Let's try to add game features and see how the score changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players[\"darkSkin\"] = df_players_description[\"darkSkin\"]\n",
    "df_players.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_forest_scores(dataframe, features, target=\"darkSkin\", estimators=10, folds=20):\n",
    "    clf = RandomForestClassifier(n_estimators=estimators)\n",
    "    X = dataframe[features]\n",
    "    y = list(dataframe[\"darkSkin\"].values)\n",
    "    \n",
    "    # Cross validation scores\n",
    "    scores = cross_val_score(clf, X, y, cv=folds, scoring='accuracy')\n",
    "    \n",
    "    # Train again to get feature importances\n",
    "    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.4)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    return scores, clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players = df_players.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible_features = ['height', 'weight', 'games', 'victories', 'ties', 'defeats', 'goals', 'yellowCards', 'yellowReds', 'redCards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores, importances = random_forest_scores(df_players, possible_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(scores).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding features adds around 3% of precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(possible_features, importances):\n",
    "    df_feature_importances = pd.DataFrame({\"features\": possible_features, \"importances\": importances})\n",
    "    df_feature_importances = df_feature_importances.set_index(\"features\")\n",
    "    df_feature_importances = df_feature_importances.sort_values(\"importances\", ascending=False)\n",
    "    df_feature_importances.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(possible_features, importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "possible_features_2 = list(possible_features)\n",
    "possible_features_2.remove(\"goals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores2, importances2 = random_forest_scores(df_players, possible_features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(scores2).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(scores).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now try to incorporate the information about racial bias in each referee's country of origin.**\n",
    "\n",
    "Previously we computed the average meanIAT and meanExp values for each player over all referee-player dyads. We again use the existing meanIAT and meanExp information from each dyad to create two new features for each player: stdIAT and stdExp. Note that this is not the same calculation as averaging the standard deviations _seIAT_ and _seExp_ from each dyad - on a high-level stdIAT and stdExp indicate the variance in referee nationalities (and potentially biases) over  all matches in which the player played.\n",
    "\n",
    "This gives us four racial bias-related features:\n",
    "- meanIAT = average of meanIAT values over all dyads containing the given player\n",
    "- meanExp = average of meanExp values over all dyads containing the given player\n",
    "- stdIAT = std deviation of meanIAT values over all dyads containing the given player\n",
    "- stdExp = std deviation of meanExp values over all dyads containing the given player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players[['stdIAT','stdExp']] = df_by_player['meanIAT', 'meanExp'].agg(np.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We check that our new standard deviation features were computed correctly (TODO improve)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players[df_players['stdExp'].isnull()]#.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players[df_players['stdIAT'].isnull()]#.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_players.replace(to_replace={'stdIAT': {np.nan : 0}, 'stdExp': {np.nan : 0}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible_features_with_bias = ['meanIAT', 'stdIAT', 'meanExp', 'stdExp', 'height', 'weight', 'games', 'victories', 'ties', 'defeats', 'goals', 'yellowCards', 'yellowReds', 'redCards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores_bias, importances_bias = random_forest_scores(df_players, possible_features_with_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(scores_bias).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(possible_features_with_bias, importances_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When we include the four features relating to the racial bias in referee countries, we see a bump of several percent in accuracy, up to 78-79%. These features are ranked highly in importance so they seem to be quite important to the prediction!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose KMeans as clustering algorithm as it is the simplest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the features corresponding to national racial bias in the referee's home countries, since in part 1 we found these to be predictive of skin color.\n",
    "\n",
    "**TODO shouldn't we try to use yellow/red cards as well?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias_features = ['meanIAT', 'stdIAT', 'meanExp', 'stdExp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df_players[bias_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run KMeans with the default k-means++ initialization strategy. We're looking for two clusters corresponding to dark skinned and light skinned players respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2).fit(X)\n",
    "cluster_indexes = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster1 = df_players.loc[cluster_indexes == 0]\n",
    "cluster2 = df_players.loc[cluster_indexes == 1]\n",
    "\n",
    "def dark_skin_proportion(cluster):\n",
    "    \"\"\"Return the proportion of dark skin player in the cluster\"\"\"\n",
    "    return cluster['darkSkin'].sum() / len(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dark_skin_proportion(cluster1), dark_skin_proportion(cluster2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_dark_separator_score(cluster1, cluster2):\n",
    "    \"\"\"Returns a heuristic that gives a score between 0 and 1 to the cluster assignment.\n",
    "    The higher the score, the more the clusters separate players by skin colors\"\"\"\n",
    "    return np.abs(dark_skin_proportion(cluster1) - dark_skin_proportion(cluster2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_dark_separator_score(cluster1, cluster2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmean_dark_separator_score(X):\n",
    "    \"\"\"Given a dataset, returns heuristic score and silhouette score for the\n",
    "    clusters generated by KMeans\"\"\"\n",
    "    kmeans = KMeans(n_clusters=2).fit(X)\n",
    "    cluster_indexes = kmeans.predict(X)\n",
    "    cluster1 = df_players.loc[cluster_indexes == 0]\n",
    "    cluster2 = df_players.loc[cluster_indexes == 1]\n",
    "    \n",
    "    \n",
    "    return cluster_dark_separator_score(cluster1, cluster2), silhouette_score(X, cluster_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximize score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at all 1-feature cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_score_silhouette = []\n",
    "\n",
    "for feature in possible_features_with_bias:\n",
    "    score, silhouette = kmean_dark_separator_score(df_players[[feature]])\n",
    "    feature_score_silhouette.append((feature, score, silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the 1-feature case, the silhouette is always between 0.5-0.75 which is quite good. We thus order by the score they give when they are used alone in KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_score_silhouette = sorted(feature_score_silhouette, key=lambda x:x[1])\n",
    "[\"Feature: {:12} Score: {:.2f}    Silhouette: {:.2f}\".format(feature, score, silhouette) \n",
    "     for feature, score, silhouette in reversed(feature_score_silhouette)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We order the features by score they give as a single KMean feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ordered_features = [fscore[0] for fscore in feature_score_silhouette]\n",
    "\",\".join(ordered_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove each feature iteratively and print the resulting heuristic score and silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove each feature iteratively\n",
    "for i in range(len(ordered_features)):\n",
    "    score, silhouette = kmean_dark_separator_score(df_players[ordered_features[i:]])\n",
    "    \n",
    "    print('Before removing {:12} score is {:.02f}  silhouette is {:.02f}'.format(ordered_features[i], score, silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that keeping only `[\"meanIAT\", \"stdIAT\"]` is optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df_players[[\"meanIAT\", \"stdIAT\"]]\n",
    "kmeans = KMeans(n_clusters=2).fit(X)\n",
    "cluster_indexes = kmeans.predict(X)\n",
    "cluster1 = df_players.loc[cluster_indexes == 0]\n",
    "cluster2 = df_players.loc[cluster_indexes == 1]\n",
    "\n",
    "FMT = \"Cluster {}: {:.2f}% of dark skin players\"\n",
    "print(FMT.format(1, 100 * dark_skin_proportion(cluster1)))\n",
    "print(FMT.format(2, 100 * dark_skin_proportion(cluster2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the clustering works decently, one cluster contains **a vast majority of dark skin players** while the other contains a **vast majority of light skin players**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the silhouette score is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"Silhouette score: {:.2f}\".format(silhouette_score(X, cluster_indexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We thus find that using features `[\"meanIAT\", \"stdIAT\"]` gives two clusters with good silhouette score, and separates quite well dark-skin and light-skin players**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
