{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import string\n",
    "\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Loading and cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we need to load the emails into a dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Emails.csv into pandas\n",
    "emails_dir = \"hillary-clinton-emails\"\n",
    "emails_csv_filename = \"Emails.csv\"\n",
    "\n",
    "emails_path = path.join(emails_dir, emails_csv_filename)\n",
    "\n",
    "emails_df = pd.read_csv(emails_path)\n",
    "\n",
    "# Take a peek\n",
    "emails_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We want to create our wordcloud from the column(s) that contain(s) the full text of the emails**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Which columns of the dataframe contain the text we want for our wordcloud?\n",
    "emails_df.columns\n",
    "\n",
    "#I found:\n",
    "#- ExtractedSubject\n",
    "#- ExtractedBodyText\n",
    "#- RawText\n",
    "\n",
    "# TODO SV check these columns to see if they're actually filled in and make sense\n",
    "emails_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO I decided to use the RawText for the wordcloud and might use ExtractedSubject and ExtractedBodyText in later steps.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Generating wordclouds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Concatenate all raw text\n",
    "concatenated_raw_text = ' '.join(emails_df['RawText'])\n",
    "len(concatenated_raw_text)\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud().generate(concatenated_raw_text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The wordcloud library helpfully removed common english stopwords, but we still have some boring terms like 'date' and 'subject' in our cloud.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can improve the results by doing some pre-processing:**\n",
    "- tokenization\n",
    "- stopword removal\n",
    "- lemmatization\n",
    "- stemming\n",
    "- remove punctuation & single char tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#NOTE to team you should run nltk.download() but maybe outside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Takes a few seconds to run:\n",
    "tokenized = nltk.word_tokenize(concatenated_raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's remove some more stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "email_stopwords = nltk.corpus.stopwords.words('english')\n",
    "additional_stopwords = ['subject', 'date', 're', 'cc', 'bcc', 'fwd', 'fw', 'sent', 'mr', 'mrs']\n",
    "email_stopwords.extend(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_without_stopwords = [t.lower() for t in tokenized if t.lower() not in email_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization - we use the WordnetLemmatizer to group inflected forms of a word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(t) for t in tokens_without_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming - we use the porter stemmer to normalize our text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "processed_tokens = [porter.stem(t) for t in lemmatized_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove punctuation & single character token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(token):\n",
    "    for c in string.punctuation:\n",
    "        token = token.replace(c, \"\")\n",
    "    \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_tokens = [token for token in processed_tokens if len(token) > 1]\n",
    "processed_tokens = [remove_punctuation(token) for token in processed_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All processing at once. We don't apply stemming on emails, since it would destroy most country names\n",
    "def process_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lowered = (t.lower() for t in tokens)\n",
    "    processed = (lemmatizer.lemmatize(t) for t in lowered if t not in email_stopwords)\n",
    "    processed = (remove_punctuation(t) for t in processed)\n",
    "    processed = (t for t in processed if len(t) > 1)\n",
    "            \n",
    "    return list(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud2 = WordCloud().generate(\" \".join(processed_tokens))\n",
    "\n",
    "plt.imshow(wordcloud2)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The wordcloud looks quite a bit different - now we can see some interesting common words like \"Cheryl\" \"Abedin\" etc. This approach has some pros and cons:**\n",
    "- (+) we were able to remove some of the obviously common words\n",
    "- (-) stemming doesn't seem that useful\n",
    "- (-) removing words is sort of an iterative process as each time we generate the wordcloud we see new frequent words to exclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. World country mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries = {country: 0 for country in pycountry.countries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_country_tokens = set()\n",
    "concatenated_body_text = \" \".join(str(email) for email in emails_df[\"ExtractedBodyText\"])\n",
    "\n",
    "for token in process_text(concatenated_body_text):\n",
    "    if token not in bad_country_tokens:\n",
    "        try:\n",
    "            country = pycountry.countries.lookup(token)\n",
    "            countries[country] += 1\n",
    "        except LookupError:\n",
    "            bad_country_tokens.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_df = pd.DataFrame.from_dict({country.name: occurence for (country, occurence) in countries.items()}, orient=\"index\")\n",
    "country_df.columns = [\"Occurences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_df.sort_values(by=\"Occurences\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pycountry.countries.lookup(\"PM\").name)\n",
    "print(pycountry.countries.lookup(\"AM\").name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saint Pierre and Miquelon refers to \"PM\". \"AM\" was already in the stop-words, that's why we don't have problems with Armenia. We could start again with \"pm\" as a stopword, but Saint Pierre and Miquelon is very likely not to appear in any email.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_df.loc[\"Saint Pierre and Miquelon\"] = 0\n",
    "country_df.sort_values(by=\"Occurences\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vader = nltk.sentiment.vader.SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vader.polarity_scores(\"I love you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vader.polarity_scores(\"I hate you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "email_stopwords.append(\"pm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "\n",
    "# Create cache folder\n",
    "CACHE_DIR = \"cache\"\n",
    "CACHE_PATH = os.path.join(CACHE_DIR, \"country_sentiment_df.bak\")\n",
    "\n",
    "if not os.path.exists(CACHE_DIR):\n",
    "    os.makedirs(CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_sentiment_list = []\n",
    "\n",
    "# !! If not in the cache, takes ~10 min to run\n",
    "if os.path.isfile(CACHE_PATH):\n",
    "    with open(CACHE_PATH, 'rb') as handle:\n",
    "        country_sentiment_df = pickle.load(handle)\n",
    "\n",
    "else:\n",
    "    for (index, email) in emails_df.iterrows():\n",
    "        text = email[\"ExtractedBodyText\"]\n",
    "        words = process_text(str(text))\n",
    "\n",
    "        for word in words:\n",
    "            if word not in bad_country_tokens:\n",
    "                country = pycountry.countries.lookup(word)\n",
    "                sentiment = vader.polarity_scores(text)[\"compound\"]\n",
    "                country_sentiment_list.append([country.name, sentiment])\n",
    "                break\n",
    "\n",
    "    country_sentiment_df = pd.DataFrame(country_sentiment_list, columns=[\"Country\", \"Sentiment\"])\n",
    "    \n",
    "    with open(CACHE_PATH, 'wb') as handle:\n",
    "        pickle.dump(country_sentiment_df, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_by_country = country_sentiment_df.groupby(\"Country\")\n",
    "\n",
    "\n",
    "sentiment_occurence_df = sentiment_by_country.count().join(sentiment_by_country.mean(), lsuffix='_left')\n",
    "sentiment_occurence_df.columns = [\"Frequency\", \"Sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, a full & difficult to read plot of sentiment towards countries. Then, we only show countries with most positive / most negative sentiment. We can see that the emails globally have a positive sentiment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"coolwarm_r\", n_colors=len(sentiment_occurence_df))\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "sentiment_occurence_df[\"Sentiment\"].sort_values().plot(kind=\"bar\", color=palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (8, 4)\n",
    "\n",
    "# Most negative sentiment\n",
    "sentiment_occurence_df[\"Sentiment\"].sort_values().head(25).plot(kind=\"bar\", color=palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most positive sentiment\n",
    "sentiment_occurence_df[\"Sentiment\"].sort_values().tail(25).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim.models.ldamodel as ldamodel\n",
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [process_text(str(email)) for email in emails_df[\"ExtractedBodyText\"]]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pretty_print_topics(lda):\n",
    "    for topics in lda.show_topics(num_topics=20, num_words=8, formatted=False):\n",
    "        print([topic[0] for topic in topics[1]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !! Takes a few minutes\n",
    "\n",
    "for topics_count in range(5, 50, 10):\n",
    "    print(\"{} topics\".format(topics_count))\n",
    "    pretty_print_topics(ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=topics_count))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It looks like 45 topics gives the most meaningful results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
