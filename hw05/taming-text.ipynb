{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import string\n",
    "\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.sentiment\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Loading and cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we need to load the emails into a dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Emails.csv into pandas\n",
    "emails_dir = \"hillary-clinton-emails\"\n",
    "emails_csv_filename = \"Emails.csv\"\n",
    "\n",
    "emails_path = path.join(emails_dir, emails_csv_filename)\n",
    "\n",
    "emails_df = pd.read_csv(emails_path)\n",
    "len(emails_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a peek\n",
    "emails_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We want to create our wordcloud from the column(s) that contain(s) the full text of the emails. From the schema, we know that the email subject should be in ExtractedSubject and email text should be in ExtractedBodyText. But we should make sure these fields are filled in!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Which columns of the dataframe contain the text we want for our wordcloud?\n",
    "#I found:\n",
    "#- ExtractedSubject\n",
    "#- ExtractedBodyText\n",
    "#- RawText\n",
    "\n",
    "# check these columns to see if they're actually filled in and make sense\n",
    "emails_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most rows have non-null ExtractedSubject and ExtractedBodyText, so we decided arbitrarily to drop all the emails that are missing these fields.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In-place!\n",
    "emails_df.dropna(subset=['ExtractedSubject', 'ExtractedBodyText'], inplace=True)\n",
    "len(emails_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Generating wordclouds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll generate a wordcloud using the concatenated text of the entire email corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Concatenate all raw text\n",
    "concatenated_body_text = ' '.join(emails_df['ExtractedSubject'] + ' ' + emails_df['ExtractedBodyText'])\n",
    "len(concatenated_body_text)\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud().generate(concatenated_body_text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The wordcloud library helpfully removed common english stopwords, but we still have some boring terms like 'RE' and 'FW' in our cloud!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can improve the results by doing some pre-processing:**\n",
    "- tokenization\n",
    "- stopword removal (including email terms)\n",
    "- lemmatization\n",
    "- stemming (maybe?)\n",
    "- remove punctuation & single char tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#NOTE to team you should run nltk.download() but maybe outside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Takes a few seconds to run:\n",
    "tokenized = nltk.word_tokenize(concatenated_body_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's remove some more stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define annoying email terms as stopwords\n",
    "email_stopwords = nltk.corpus.stopwords.words('english')\n",
    "additional_stopwords = ['subject', 'date', 're', 'cc', 'bcc', 'fwd', 'fw', 'sent', 'mr', 'mrs']\n",
    "email_stopwords.extend(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_without_stopwords = [t.lower() for t in tokenized if t.lower() not in email_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization - we use the WordnetLemmatizer to group inflected forms of a word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(t) for t in tokens_without_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming - we can use the porter stemmer to normalize our text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "processed_tokens = [porter.stem(t) for t in lemmatized_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove punctuation & single character token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(token):\n",
    "    for c in string.punctuation:\n",
    "        token = token.replace(c, \"\")\n",
    "    \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_tokens = [token for token in processed_tokens if len(token) > 1]\n",
    "processed_tokens = [remove_punctuation(token) for token in processed_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our NLTK pipeline (fully defined below) consists of tokenization, lower-case conversion, lemmatization and punctuation/single char removal.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All processing at once. We don't apply stemming on emails, since it would destroy most country names\n",
    "def process_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lowered = (t.lower() for t in tokens)\n",
    "    processed = (lemmatizer.lemmatize(t) for t in lowered if t not in email_stopwords)\n",
    "    processed = (remove_punctuation(t) for t in processed)\n",
    "    processed = (t for t in processed if len(t) > 1)\n",
    "            \n",
    "    return list(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc2_processed_tokens = process_text(concatenated_body_text)\n",
    "wordcloud2 = WordCloud().generate(\" \".join(wc2_processed_tokens))\n",
    "\n",
    "plt.imshow(wordcloud2)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The second wordcloud looks a bit better thanks to the additional processing. Some conclusions:**\n",
    "- (+) we were able to remove some of the mundane email terms from the text\n",
    "- (~) lemmatization seemed more useful than stemming for generating a readable wordcloud.\n",
    "- (-) removing words is sort of an iterative process as each time we generate the wordcloud we see new frequent words that we'd prefer to exclude!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. World country mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We use pycountry to find references in the emails to various countries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries = {country: 0 for country in pycountry.countries}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we go through all of the tokens and look for any mentions of countries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_country_tokens = set()\n",
    "concatenated_body_text = \" \".join(str(email) for email in emails_df[\"ExtractedBodyText\"])\n",
    "\n",
    "for token in process_text(concatenated_body_text):\n",
    "    if token not in bad_country_tokens:\n",
    "        try:\n",
    "            country = pycountry.countries.lookup(token)\n",
    "            countries[country] += 1\n",
    "        except LookupError:\n",
    "            bad_country_tokens.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_df = pd.DataFrame.from_dict({country.name: occurence for (country, occurence) in countries.items()}, orient=\"index\")\n",
    "country_df.columns = [\"Occurences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_df.sort_values(by=\"Occurences\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pycountry.countries.lookup(\"PM\").name)\n",
    "print(pycountry.countries.lookup(\"AM\").name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saint Pierre and Miquelon refers to \"PM\". \"AM\" was already in the stop-words, that's why we don't have problems with Armenia. We could start again with \"pm\" as a stopword, but Saint Pierre and Miquelon is very likely not to appear in any email.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_df.loc[\"Saint Pierre and Miquelon\"] = 0\n",
    "country_df.sort_values(by=\"Occurences\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vader = nltk.sentiment.vader.SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vader.polarity_scores(\"I love you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vader.polarity_scores(\"I hate you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "email_stopwords.append(\"pm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "\n",
    "# Create cache folder\n",
    "CACHE_DIR = \"cache\"\n",
    "CACHE_PATH = os.path.join(CACHE_DIR, \"country_sentiment_df.bak\")\n",
    "\n",
    "if not os.path.exists(CACHE_DIR):\n",
    "    os.makedirs(CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_sentiment_list = []\n",
    "\n",
    "# !! If not in the cache, takes ~10 min to run\n",
    "if os.path.isfile(CACHE_PATH):\n",
    "    with open(CACHE_PATH, 'rb') as handle:\n",
    "        country_sentiment_df = pickle.load(handle)\n",
    "\n",
    "else:\n",
    "    for (index, email) in emails_df.iterrows():\n",
    "        text = email[\"ExtractedBodyText\"]\n",
    "        words = process_text(str(text))\n",
    "\n",
    "        for word in words:\n",
    "            if word not in bad_country_tokens:\n",
    "                country = pycountry.countries.lookup(word)\n",
    "                sentiment = vader.polarity_scores(text)[\"compound\"]\n",
    "                country_sentiment_list.append([country.name, sentiment])\n",
    "                break\n",
    "\n",
    "    country_sentiment_df = pd.DataFrame(country_sentiment_list, columns=[\"Country\", \"Sentiment\"])\n",
    "    \n",
    "    with open(CACHE_PATH, 'wb') as handle:\n",
    "        pickle.dump(country_sentiment_df, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_by_country = country_sentiment_df.groupby(\"Country\")\n",
    "\n",
    "\n",
    "sentiment_occurence_df = sentiment_by_country.count().join(sentiment_by_country.mean(), lsuffix='_left')\n",
    "sentiment_occurence_df.columns = [\"Frequency\", \"Sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, a full & difficult to read plot of sentiment towards countries. Then, we only show countries with most positive / most negative sentiment. We can see that the emails globally have a positive sentiment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"coolwarm_r\", n_colors=len(sentiment_occurence_df))\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "sentiment_occurence_df[\"Sentiment\"].sort_values().plot(kind=\"bar\", color=palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is a plot of the countries towards which Hillary & co. expressed the most negative sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (8, 4)\n",
    "\n",
    "# Most negative sentiment\n",
    "sentiment_occurence_df[\"Sentiment\"].sort_values().head(25).plot(kind=\"bar\", color=palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarly, here is a plot of the countries towards which Hillary & co. expressed the most positive sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most positive sentiment\n",
    "sentiment_occurence_df[\"Sentiment\"].sort_values().tail(25).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: There are some surprising countries at the top of the sentiment list!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim.models.ldamodel as ldamodel\n",
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For this task we use the full emails (including subject), process the text with our NLTK piepline, then create LDA modes with varying numbers of topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [process_text(str(email)) for email in emails_df['ExtractedSubject'] + \" \" + emails_df[\"ExtractedBodyText\"]]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pretty_print_topics(lda):\n",
    "    for topics in lda.show_topics(num_topics=20, num_words=8, formatted=False):\n",
    "        print([topic[0] for topic in topics[1]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !! Takes a few minutes\n",
    "models = dict()\n",
    "for topics_count in range(5, 50, 10):\n",
    "    print(\"{} topics\".format(topics_count))\n",
    "    models[topics_count] = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=topics_count)\n",
    "    pretty_print_topics(models[topics_count])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's very difficult to tell without more knowledge of the email contents, but it looks like 45 topics gives the most meaningful results. There are some pretty stable topics across the different iterations, for example Israeli-Arab-Palestinian relations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
